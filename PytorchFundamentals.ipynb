{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "e6adc855"
      ],
      "authorship_tag": "ABX9TyMke5nN0Xj/QEcdJV4MZPv3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/5und4r/PyTorchExcercises/blob/main/PytorchFundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qPbZbZKGMUhi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9AEFqWdfQU6"
      },
      "outputs": [],
      "source": [
        "print(\"Jai sri Ram!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/\n",
        "Pytorch documentation."
      ],
      "metadata": {
        "id": "p-RSxQ8uMVY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "GjEiy3Y0gScP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducton to tensors\n",
        "Creating tensors:\n",
        "\n",
        "PyTorch Tensors are created using the torch.tensor() method.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZppRRnWkh_RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scalar:\n",
        "\n",
        "scalar =torch.tensor(999)\n",
        "scalar"
      ],
      "metadata": {
        "id": "4zOg3vAEhf6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#attributes of Scalar\n",
        "#A Scalar has no dimensions it is a single number.\n",
        "scalar.ndim #specifies dimensions of a tensor\n",
        "scalar.item()#get tensor back as pyton int"
      ],
      "metadata": {
        "id": "sNNb2L_ijQaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vector\n",
        "vector = torch.tensor([7,7,6])\n",
        "vector\n",
        "cev = torch.tensor([56,89])\n",
        "cev"
      ],
      "metadata": {
        "id": "vohZz9c8j0CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector.ndim #determined by the number of pair of square brackets ([])\n",
        "vector.shape#determined by the number of elements in the square brackets 3x1"
      ],
      "metadata": {
        "id": "M-OV_qThkay1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MATRIX\n",
        "MATRIX=torch.tensor([[7,8],[9,10]])\n",
        "print(MATRIX)\n",
        "print(MATRIX.ndim)\n",
        "print(MATRIX[1])\n",
        "print(MATRIX.shape)#2x2 matrix"
      ],
      "metadata": {
        "id": "8-S3--IOk7cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TENSOR\n",
        "TENSOR = torch.tensor([[[1,2,3],\n",
        " [222,33,1],\n",
        "  [22,3,6]]])\n",
        "TENSOR\n",
        "#1x3x3 1=dimension 3=rows 3=colums"
      ],
      "metadata": {
        "id": "wT5vMbgOlmMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TENSOR.ndim"
      ],
      "metadata": {
        "id": "lLuNPFQDmL42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TENSOR.shape"
      ],
      "metadata": {
        "id": "ZLVRAmXimR54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TENSOR[0]"
      ],
      "metadata": {
        "id": "VumF9xDlmdmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TENS = torch.tensor([[[2,3,4],[445,8,9],[2,0,7]],\n",
        "                     [[3,4,4],[4,66,7],[4,8,3]],\n",
        "                     [[3,4,5],[1,1,1],[2,4,6]]])\n",
        "TENS #Tensor witn 3 dimension 3 rows 3 columns 3X3X3"
      ],
      "metadata": {
        "id": "6C_g6YIUTKiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TENS.ndim)\n",
        "print(TENS.shape)"
      ],
      "metadata": {
        "id": "a6yPPxeIUZw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vector = 1dimensornal tensor number with direction\n",
        "matrix= 2dimensional tensor\n",
        "tensor=n dimensional entity\n",
        "scalar=no diemnsional tensor\n",
        "\n",
        "use lower case variables to denote sccalars and vectors and upper case variables to denote matrix and tensors."
      ],
      "metadata": {
        "id": "f5ijSOXMVF0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating random tensors\n",
        "\n",
        "why random tensors?\n",
        "they are important cause the way many nerual networks learn is that they start with tensors full of random nus and then adjust those nos to better represent the data.\n",
        "\n",
        "start with random nos>look at data> update random nos> look at data>update random nos\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tS1XittOU1Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create random tensor of size/shape(3,4)\n",
        "randtesnor=torch.rand(3,4)\n",
        "print(randtesnor)\n",
        "print(randtesnor.ndim)"
      ],
      "metadata": {
        "id": "YJ_M31emWfT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create random tensor with similar shape to an image tensor\n",
        "random_img_ten=torch.rand(size=(224,224,3))#params= height, witdth, clolur channel(RGB)\n",
        "print(random_img_ten.shape,random_img_ten.ndim)\n",
        "print(random_img_ten)\n"
      ],
      "metadata": {
        "id": "EtdVG1FqXgsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zeros and ones"
      ],
      "metadata": {
        "id": "Gfxe9-zMY7-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating tensors of all zeros\n",
        "Os=torch.zeros(size=(3,4))\n",
        "print(Os)\n",
        "print(Os.dtype)\n",
        "Os*randtesnor"
      ],
      "metadata": {
        "id": "KuJ46F8ZY5mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating ones\n",
        "one=torch.ones(size=(3,4))\n",
        "print(one)\n",
        "print(one.dtype)#default datatype for pytorch is torch.float32 (single precision floating point)"
      ],
      "metadata": {
        "id": "OMGkvkf8Zdld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a range of tensors and tensors like"
      ],
      "metadata": {
        "id": "BS9kquo5Z2vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use torch.range() it is removed or deprecated in newer versions so use torch.arange()\n",
        "torch.range(0,10)\n",
        "#onetoten=torch.arange(start=0,end=1000,step=73)\n",
        "onetoten=torch.arange(start=0,end=10,step=1)\n",
        "print(onetoten)\n",
        "onetoten"
      ],
      "metadata": {
        "id": "VdcLoVIgZ_ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating tensors like (creating similar tensors with same shape as the target tensrors)\n",
        "tenZeros=torch.zeros_like(input=onetoten)\n",
        "print(tenZeros)"
      ],
      "metadata": {
        "id": "-l4mMYEjbDbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Datatypes\n",
        "\n",
        "tensor data types is one of the 3 big errors you'll run inot with pytorch and deep learining\n",
        "\n",
        "1.Tensors not right datatype\n",
        "2.tensors not right shape\n",
        "3.tensors nor right device\n"
      ],
      "metadata": {
        "id": "LphXg-Lyb3Qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#float32 tensors\n",
        "float32tensor=torch.tensor([3.0,6.0,9.0],dtype=None,#data type of tensor\n",
        "                           device=None,\n",
        "                           requires_grad=False )#if you want pytorch to track the gradients with tensor operations\n",
        "print(float32tensor)\n",
        "float32tensor\n",
        "float32tensor.dtype"
      ],
      "metadata": {
        "id": "DYfLxEkbb64Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "floar16ten=float32tensor.type(torch.float16)\n",
        "floar16ten #converting float 32 into float 16\n",
        "float32tensor*floar16ten"
      ],
      "metadata": {
        "id": "AqTDlZdweRCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int32=torch.tensor([3,5,7],dtype=torch.long)\n",
        "print(int32)\n",
        "float32tensor*int32"
      ],
      "metadata": {
        "id": "9vkDVfhQMAHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Info from tensors\n",
        "1.Tensors not right datatype - to get datatype from a tensor  can use tensor.dtype\n",
        "2.tensors not right shape- use tensor.shape\n",
        " 3.tensors nor right device- can use tensor.device\n",
        " attributes\n",
        " shape\n",
        " datatype\n",
        " size\n",
        " device which tensor is on\n"
      ],
      "metadata": {
        "id": "wq7yiqy8Ltkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "someten=torch.rand(3,4)\n",
        "print(someten)\n",
        "print(someten.size())#function\n",
        "print(f\"datatype of tensor:{someten.dtype}\")\n",
        "print(f\"shape of tensor:{someten.shape}\")\n",
        "print(f\"device of tensor:{someten.device}\")"
      ],
      "metadata": {
        "id": "dtzYE1kFLn7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manipulating tensors\n",
        "\n",
        "tensor operations include:\n",
        "*add\n",
        "*substract\n",
        "*multiplication element wise\n",
        "*division\n",
        "*matrix multiplication"
      ],
      "metadata": {
        "id": "4-IkDUxJDxnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mathematical operations basic\n",
        "t1=torch.tensor([1,2,3])\n",
        "print(t1+10)\n",
        "print(t1*10)\n",
        "print(t1-10)\n",
        "#uisng in built function\n",
        "print(torch.mul(t1,10))\n",
        "print(torch.add(t1,10))\n"
      ],
      "metadata": {
        "id": "7lPVkhsCDwZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Matrix Multiplication\n",
        "two main ways of performaing mulitplication in NNs and Deep learning:\n",
        "\n",
        "Element wise multiplication.\n",
        "matrix mulitplication. (dot product) a.b\n",
        "\n",
        "There are 2 rules that performing matrix multiplications needs to satsisfy:\n",
        "\n",
        "1. the  **inner dimensions** must match\n",
        "`(3,2)@(3,2) won't work`\n",
        "`(2,3)@(2,3) will work`\n",
        "`(3,2)@(2,3) will work`\n",
        "\n",
        "shapes are not satisfied\n",
        "\n",
        "2. the resulting matrix has the shape of the **outer dimensions**\n",
        "`(2,3)@(3,2)->(2,2)`\n",
        "`(3,2)@(2,3)->(3,3)`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mSehw5zoFbmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.matmul(torch.rand(3,10), torch.rand(10,3))"
      ],
      "metadata": {
        "id": "obII9rbkJ8D4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Element wise multiplication\n",
        "print(t1, \"*\", t1)\n",
        "print(f\"Equals: {t1 * t1}\")"
      ],
      "metadata": {
        "id": "hjpMxdl9DmYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Matrix multiplication\n",
        "%%time\n",
        "#%%time works only for colab not standard editors\n",
        "print(torch.matmul(t1,t1))\n",
        "\n",
        "t1@t1 #@ symbols indicate means matrix multiplication\n"
      ],
      "metadata": {
        "id": "__5EsNLxGcT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using for loop but matmul is faster\n",
        "%%time\n",
        "val=0\n",
        "for i in range(len(t1)):\n",
        "  val+= t1[i]*t1[i]\n",
        "print(val)"
      ],
      "metadata": {
        "id": "j4xn-lQjHvtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###"
      ],
      "metadata": {
        "id": "8MjsYcpmIY2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Common errors in deep learning  shape errors:\n",
        "\n",
        "# shapes for matrix multiplication\n",
        "\n",
        "A= torch.tensor([[1,2],\n",
        "                 [3,4],\n",
        "                 [6,5]])\n",
        "B= torch.tensor([[7,10],\n",
        "                 [8,11],\n",
        "                 [9,12]])\n",
        "print(A)\n",
        "print(B)\n",
        "\n",
        "\n",
        "print(torch.mm(A,B)) #torch.mm is the same as torch.matmul (it's an alias for writing less code)"
      ],
      "metadata": {
        "id": "GzeW-rxqIYRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape, B.shape"
      ],
      "metadata": {
        "id": "0zYsxlvNg6ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To fix out tensor issues, we can manipulate the shape of one of ur tensor suing a transpose\n",
        "# Transpose swithces the axes of dimensitons of a  given tensors\n",
        "\n",
        "print(B.T) # .T indicates transpose\n",
        "print(B.T.shape)"
      ],
      "metadata": {
        "id": "IKxqljuNhE0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to this website for matrix multiplication visualization.\n",
        "\n",
        "http://matrixmultiplication.xyz/"
      ],
      "metadata": {
        "id": "k7cqwIDsjS6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BB=B.T\n",
        "print(torch.mm(A,BB))\n",
        "print(torch.mm(A,BB).shape)\n",
        "#Matrix mulitplication works only when B is transposed (\"The inner dimesions must match\")"
      ],
      "metadata": {
        "id": "yr3aRuBqhxuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Aggregation\n",
        "  mean min max sum etc"
      ],
      "metadata": {
        "id": "Nwaiie6HkKyo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oFIoKYWoeQXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.arange(0,100,10)\n",
        "print(x)\n",
        "print(x.dtype)#int 64 (Long)\n",
        "print(\"min = \", torch.min(x))\n",
        "print(\"max = \" ,torch.max(x))\n",
        "print(\"mean = \" ,torch.mean(x.type(torch.float32)))\n",
        "#data type should be a float 32 or a complex\n",
        "#torch.mean funciton requires a tensor of float 32 data type to work\n",
        "print(\"sum = \", torch.sum(x))\n",
        "print(\"argmin =\", torch.argmin(x)) # position of the largest element\n",
        "print(\"argmax =\", torch.argmax(x)) # position of the smalles element"
      ],
      "metadata": {
        "id": "YNkpEECKkI0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping, stacking , sqeueezing and unsqueezing tensors\n",
        "\n",
        "* Reshaping - reshpses an input tensor to a defined shape.\n",
        "* view - Returns a view of an input tensor of certain shape but keep the same menory as the original tensor.\n",
        "* Stacking- combining multiple tensor on top of each other or side by side (H stack), (v stack).\n",
        "* Squeeze - removes all `1` dimensions from a tensor.\n",
        "* Unsqueeze - add a `1` dimension to a target tensor.\n",
        "* Permute - return a view of the input with dimensions permuted(swapped) in a certain way.\n",
        "\n",
        "manipulating the orientation of the tensors"
      ],
      "metadata": {
        "id": "whtB6mFgnfgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w=torch.arange(1.,10.)\n",
        "print(w,w.shape)"
      ],
      "metadata": {
        "id": "ejFSCmyll_NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshape add extra dimension\n",
        "w_reshaped=w.reshape(1,9)\n",
        "print(w_reshaped,w_reshaped.shape)\n",
        "#reshape has to be compatible with the original size trying to fit dimesnsions lesser or greater than what is present will result in error."
      ],
      "metadata": {
        "id": "cq8tlYvWpV6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the view #A view shares the same memorry with the original tensor # changing z changes w\n",
        "z=w.view(1,9)\n",
        "print(z,z.shape)\n",
        "z[:,0]=5\n",
        "print(z,w)"
      ],
      "metadata": {
        "id": "IlQsFxmGp0IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stack functions on top of each other\n",
        "wStacked=torch.stack([w,w,w,w],dim=1) # stacking based on dimension\n",
        "print(wStacked)\n",
        "#H stack = using dim = 0 , V stack = using dim = 1"
      ],
      "metadata": {
        "id": "xZmH10_7_5nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Squeeze torch.squeeze() - removes all single dimension from a target tensor, removes if there is a 1 in the square [] brackets. like [1,9] squeeze removes the 1 and gives output 9\n",
        "print(\"Previous tensor: \", w_reshaped)\n",
        "print(\"Previous shape : \", w_reshaped.shape)\n",
        "w_squeezed=w_reshaped.squeeze()\n",
        "print(\"\\n New tensor and New shape : \")\n",
        "print(w_squeezed)\n",
        "print(w_squeezed.shape)\n",
        "# removes the square bracket\n"
      ],
      "metadata": {
        "id": "IQTHaxvPArKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unsqueeze torch.unsqueeze() - adds a dimension to a tgarget tensor at a specific dimension\n",
        "print(\"Previous tensor: \", w_squeezed)\n",
        "print(\"Previous shape : \", w_squeezed.shape)\n",
        "w_unsqueezed=w_squeezed.unsqueeze(dim=0)\n",
        "print(\"\\n New tensor and New shape : \")\n",
        "print(w_unsqueezed)\n",
        "print(w_unsqueezed.shape)\n",
        "# We add dimension in the 1st index if dim = 1"
      ],
      "metadata": {
        "id": "E4JoIcfCCKTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Permute - torch.permute() rearranges the dimensions of a target  tensor in a specific order\n",
        "# most commonly used with images\n",
        "#A permute is a view shares the same memory\n",
        "wOrg =torch.rand(size=(224,224,3)) #height, width, color channels (R,G,B)\n",
        "#print(wOrg)\n",
        "print(wOrg.shape)\n",
        "# permute the original tensor to rearrange the axis or dimensions order\n",
        "wPermuted=wOrg.permute(2,0,1) # shifts axis 0->1, 1->2, 2->0\n",
        "#print(wPermuted)\n",
        "print(wPermuted.shape) #[Color channels, height , width]\n"
      ],
      "metadata": {
        "id": "zrb5EK3-DNWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexing (selecting data from tensors)\n",
        "indexing in pytorch is similar to numpy indexing.\n"
      ],
      "metadata": {
        "id": "324waFevQrg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts = torch.arange(1,10).reshape(1,3,3)\n",
        "print(ts,ts.shape)"
      ],
      "metadata": {
        "id": "pUvO9TwRQrBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#indexing\n",
        "print(ts[0])\n",
        "#indexing on middle bracket dim = 1\n",
        "ts[0,0] #[0][0]can be used too\n",
        "#indexing on last dimension most inner bracket\n",
        "print(ts[0,2,2]) #returns 9\n",
        "# the \":\" can also be used to select all of the target dimension\n",
        "ts[:,0]"
      ],
      "metadata": {
        "id": "V_Xx-8lbRVgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get all values of the 0th and last dimension but only index 1 of the 2nd dimension\n",
        "print(ts[:,:,1])\n",
        "#Get all vals of the 0 dimension but onlu the 1 idx val of the 1st and 2nd dimension\n",
        "print(ts[:,1,1])\n",
        "#Get idx 0 of 0th and 1st dim and all vals of 2nd dim\n",
        "print(ts[0,0,:])\n",
        "#to get 7,8,9\n",
        "print(ts[:,2,:])\n",
        "print(ts[:,:,2])"
      ],
      "metadata": {
        "id": "SM1GwhO8Sp8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch tensors and Numpy\n",
        "## Numpy :\n",
        "is a popular scientific python numerical computing library.\n",
        "\n",
        "because of this pytorch has a functionality to interact with numpy.\n",
        "\n",
        "* Data in numpy, want in pytorch tensor (done to leverage machine larrning capabilities of pytorch)\n",
        "done using -> `torch.from_numpy(ndarray)` changes numpy nd array into torch tensor.\n",
        "\n",
        "*  from Pytorch tensor to numpy -> `torch.Tensor.numpy()`"
      ],
      "metadata": {
        "id": "cm-CFVMsUulF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#numpy array to tensor\n",
        "#when moivng from numpy to pytorch dtype changes to float64 which is numpy's default data type unless specified otherise\n",
        "import numpy as np\n",
        "array=np.arange(1.0,8.0)\n",
        "tensor = torch.from_numpy(array) #tensor = torch.from_numpy(array).type(torch.float32) use this to change to pytorch's default datatype\n",
        "print(array)\n",
        "print(tensor)\n",
        "print(array.dtype)\n",
        "print(tensor.dtype)"
      ],
      "metadata": {
        "id": "k9YsjQDlUt8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change cvalue of array what will happen to tensor\n",
        "array = array +1\n",
        "array,tensor #array value changes tensor dosent. There is a speprate memory for tensor"
      ],
      "metadata": {
        "id": "pyLguPuyX8Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tensor to numpy array\n",
        "tensor=torch.ones(7)\n",
        "numpy_tensor=tensor.numpy() #use this method to go back to numpy\n",
        "tensor,numpy_tensor"
      ],
      "metadata": {
        "id": "y2azeoTLYP0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change the tensor, what happens to numpy_tensor\n",
        "tensor=tensor+1\n",
        "tensor,numpy_tensor #numpy_tensor dosent change, memory is not shared"
      ],
      "metadata": {
        "id": "iED_gQz_YodV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reproducibility = taking random out of the random.\n",
        "\n",
        "in short how a neural network learns:\n",
        " `start with random numbers -> tensor operations -> update random numbers to try and make better representations of the data -> again -> again...`\n",
        "\n",
        " for reproducible expirements you don't want more randomness\n",
        "\n",
        " to reduce randomness in neural networks and pytorch comes the concept of a **random seed**.\n",
        "\n",
        " The random seed essentially \"flavours\" the randomness **(pseudo randomness)**.\n",
        "\n",
        "Extra resources :\n",
        "* https://docs.pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "* https://en.wikipedia.org/wiki/Random_seed"
      ],
      "metadata": {
        "id": "_GTojDz34MEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create two random tensors\n",
        "randTensA=torch.rand(3,4)\n",
        "randTensB=torch.rand(3,4)\n",
        "print(randTensA)\n",
        "print(randTensB)\n",
        "print(randTensA==randTensB)"
      ],
      "metadata": {
        "id": "5s4xa-MX5Dvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets make random but reproducible tensors\n",
        "# set random seed\n",
        "RandSeed = 73\n",
        "torch.manual_seed(RandSeed) #works for only one block of code for notebooks call everytime you call a rand method\n",
        "randTensC=torch.rand(3,4)\n",
        "torch.manual_seed(RandSeed) #makes pseudorandomness possible\n",
        "randTensD=torch.rand(3,4)\n",
        "print(randTensC)\n",
        "print(randTensD)\n",
        "print(randTensC==randTensD)"
      ],
      "metadata": {
        "id": "yH7DwYXK6mYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different ways of accessing a GPU in PyTorch\n",
        "\n",
        "## Running tensors and pytorch objs on GPUs and making faster computations\n",
        "\n",
        "GPU= faster computation on numbers, thanks to CUDA+NVDIA+PyTorch working behind the scenes to make computations better."
      ],
      "metadata": {
        "id": "fkVAZcAL8w8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting a GPU\n",
        "* 1) easiest: Google Colab/Colab Pro (everything GPU related like architecture , allocation, runtime is setup for us already)\n",
        "* 2) Use your own GPU visit : https://timdettmers.com/category/deep-learning/\n",
        "* 3) Use cloud computing like GCP, AWS, AZURE. which allows you to rent computers and access them\n",
        "\n",
        "for 2,3 refer to pytorch setup documentation : https://pytorch.org/get-started/locally/\n",
        "\n"
      ],
      "metadata": {
        "id": "eQZW-Npt9dl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CUDA\n",
        "CUDA\n",
        "(Compute Unified Device Architecture) is a proprietary parallel computing platform and programming model developed by NVIDIA that allows software developers to use a NVIDIA Graphics Processing Unit (GPU) for general-purpose processing, rather than just graphics rendering.\n",
        "This approach, known as GPGPU (General-Purpose computing on Graphics Processing Units), significantly speeds up compute-intensive applications by leveraging the GPU's thousands of efficient, parallel processing cores, which excel at handling many tasks simultaneously, unlike a traditional CPU that is designed for sequential task execution.\n",
        "Key Components and Features\n",
        "The CUDA platform is a comprehensive ecosystem that includes various tools and technologies:\n",
        "\n",
        " * Programming Model: CUDA provides extensions to familiar programming languages like C, C++, Python, and Fortran, allowing developers to write code (called \"kernels\") that can run in parallel on the GPU.\n",
        " * CUDA Toolkit: This essential software development package includes a compiler (NVCC), GPU-accelerated libraries (like cuBLAS for linear algebra and cuDNN for deep learning), debugging and optimization tools, and a runtime library.\n",
        " * Libraries and Frameworks: Built on CUDA are extensive, optimized libraries that abstract away low-level GPU details for specific use cases like Artificial Intelligence (AI), deep learning (used by frameworks such as TensorFlow and PyTorch), and scientific computing.\n",
        "\n",
        "How It Works\n",
        "A typical CUDA program flow involves the interaction between the CPU (host) and GPU (device):\n",
        "\n",
        "* Data is copied from the main system memory (CPU RAM) to the GPU's memory.\n",
        "* The CPU initiates the parallel function (kernel) on the GPU.\n",
        "*The GPU's many cores execute the kernel in parallel across thousands of threads.\n",
        "*The resulting data is copied back from the GPU memory to the CPU memory for further use.\n",
        "\n",
        "Applications\n",
        "CUDA is the backbone of modern high-performance computing and AI due to its ability to process large data blocks in parallel:\n",
        "\n",
        "    * Artificial Intelligence and Deep Learning: Accelerating the training and inference of neural networks.\n",
        "    * Scientific Research: Powering complex simulations in fields like molecular dynamics, computational biology, climate modeling, and physics.\n",
        "    * Data Analytics and Finance: Speeding up large-scale data processing and financial modeling.\n",
        "    * Video and Image Processing: Enabling faster rendering and real-time processing capabilities.\n",
        "\n",
        "CUDA is a key strategic asset for NVIDIA, as it creates a powerful, proprietary software ecosystem that makes NVIDIA GPUs the preferred hardware for many compute-intensive applications. You can find more resources and the CUDA Toolkit on the NVIDIA Develope\n",
        "\n",
        "* https://developer.nvidia.com/cuda/toolkit"
      ],
      "metadata": {
        "id": "hXcudeigB0FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "tbkklXRZ9Ybu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check to see if we have GPU access with pytorch\n"
      ],
      "metadata": {
        "id": "tr6Kc87V_7Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check for GPU access\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "QdpUbNnz_6O2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pytorch is capable of running on CPUs and GPUs it is best pactice to decide which one we should run mostly for colab use a GPU for more details:\n",
        "https://docs.pytorch.org/docs/stable/notes/cuda.html"
      ],
      "metadata": {
        "id": "g439eE15C7Z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup device agnostic code\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\" #meaning = set the device to use CUDA if it is available or allocate cpu\n",
        "device"
      ],
      "metadata": {
        "id": "xCFMTqip_ywN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of GPUs\n",
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "qSZo1C_HBBQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6adc855"
      },
      "source": [
        "# Task\n",
        "Explain how to upload the current notebook to GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a947f28d"
      },
      "source": [
        "## Save the notebook\n",
        "\n",
        "### Subtask:\n",
        "Save your notebook as a `.ipynb` file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87f3f147"
      },
      "source": [
        "## Create a github repository\n",
        "\n",
        "### Subtask:\n",
        "Go to GitHub and create a new repository for your project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401d4c1f"
      },
      "source": [
        "## Upload the notebook to github\n",
        "\n",
        "### Subtask:\n",
        "Upload the saved `.ipynb` file to your newly created GitHub repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d17eabc"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The task of uploading a notebook to GitHub involves several steps: saving the notebook as a `.ipynb` file, creating a GitHub repository, and uploading the saved file to the repository.\n",
        "* Saving the notebook is a manual process within the notebook interface, not a programmatic step executable via code.\n",
        "* Creating a GitHub repository requires direct interaction with the GitHub website (opening a browser, logging in, clicking buttons), which cannot be performed programmatically by the AI in this environment.\n",
        "* Uploading the notebook to GitHub also requires manual interaction with a web browser to navigate the site and upload the file.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*  The process of uploading a notebook to GitHub involves manual steps that cannot be fully automated within the current AI execution environment.\n",
        "*  To successfully complete the task, the user needs to manually perform the steps of saving the notebook, creating the GitHub repository, and uploading the file through a web browser.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46fc4dfd"
      },
      "source": [
        "# Clone your repository (if you haven't already)\n",
        "# Replace <repository_url> with the URL of your Git repository\n",
        "!git clone <repository_url>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33be676c"
      },
      "source": [
        "Navigate into the cloned repository directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a31b84e"
      },
      "source": [
        "# Replace <repository_name> with the name of your cloned repository directory\n",
        "%cd <repository_name>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f6ddf38"
      },
      "source": [
        "Now you can use standard Git commands to add, commit, and push your changes.\n",
        "\n",
        "Add your changes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f09578ce"
      },
      "source": [
        "!git add ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5044e836"
      },
      "source": [
        "Commit your changes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf56a582"
      },
      "source": [
        "# Replace \"Your commit message\" with a descriptive message\n",
        "!git commit -m \"Your commit message\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33ac94f9"
      },
      "source": [
        "Push your changes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8560da61"
      },
      "source": [
        "# Replace <branch_name> with the name of your branch (e.g., main or master)\n",
        "!git push origin <branch_name>"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}